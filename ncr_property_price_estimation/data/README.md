# 99acres NCR Property Scraper

A production-ready web scraper for collecting real estate data from 99acres.com with enterprise-grade features including anti-bot evasion, checkpoint/resume capability, and graceful shutdown handling.

## Features

### üõ°Ô∏è Anti-Bot Evasion
- **Human-like behavior**: Random delays and scrolling patterns
- **Cookie support**: Maintains user session to avoid detection
- **Soft ban handling**: Automatic 2-minute cooldown on detection
- **Smart delays**: Empirically tested random intervals

### üíæ Data Management
- **Batched I/O**: Saves every 10 pages (reduces disk writes by 10x)
- **Duplicate detection**: Tracks scraped URLs across restarts
- **CSV output**: Single file for easy DVC tracking
- **In-memory buffering**: Fast data accumulation between saves

### üîÑ Reliability
- **Checkpoint/Resume**: Auto-saves progress every 10 pages
- **Graceful shutdown**: Ctrl+C saves all pending data
- **Network resilience**: Pauses on internet loss, resumes when back
- **Driver recovery**: Auto-restarts on crashes

### üìä Logging
- **Rotating file logs**: 10MB max, 5 backups
- **Dual logging**: Console (INFO) + File (DEBUG)
- **Error tracking**: Separate error log file
- **Progress tracking**: Real-time status updates

### üéØ Data Extraction

Extracts **20+ features** per property:
- **Basic**: Title, URL, City, Location, Price, Area
- **Configuration**: Bedrooms, Bathrooms, Balcony, Floor
- **Property Type**: Apartment, Villa, Builder Floor, Plot
- **Amenities**: Pool, Gym, Lift, Parking, Pooja Room, etc.
- **Details**: Facing direction, Furnishing status, Vastu compliance

## Installation

### 1. Install Dependencies

```bash
# Using pip
pip install -e .

# Or with development tools
pip install -e ".[dev]"
```

### 2. Export Cookies (Optional but Recommended)

1. Install a browser extension like "EditThisCookie" or "Cookie-Editor"
2. Visit https://www.99acres.com and log in
3. Export cookies as JSON
4. Save to `ncr_property_price_estimation/data/cookies.json`

**Why cookies?** They help the scraper appear as a logged-in user, reducing bot detection risk.

## Configuration

Edit `ncr_property_price_estimation/data/config.yaml`:

```yaml
cities:
  - name: "Noida"
    url: "https://www.99acres.com/property-in-noida-ffid?page={}"
    target_count: 8000  # Adjust per city

scraper:
  batch_size: 10  # Save every N pages
  consecutive_empty_limit: 5  # Stop after N empty pages
  coffee_break_interval: 30  # Break every N pages
  coffee_break_duration: 30  # Break duration (seconds)

logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR
```

## Usage

### Basic Usage

```bash
cd ncr_property_price_estimation/data
python ingestion.py
```

### Resume from Checkpoint

The scraper automatically resumes from the last checkpoint. Just run the same command:

```bash
python ingestion.py
```

### Graceful Shutdown

Press **Ctrl+C** to stop. The scraper will:
1. Save all pending listings from buffer
2. Save current checkpoint (city, page, count)
3. Close the browser driver
4. Exit cleanly

**No data loss!** üéâ

## Output

### Data Files

```
RealEstate_ML_Data/
‚îú‚îÄ‚îÄ 99acres_NCR_ML_Final.csv    # Main data file
‚îú‚îÄ‚îÄ checkpoint.json              # Resume checkpoint
‚îî‚îÄ‚îÄ logs/
    ‚îú‚îÄ‚îÄ scraper.log              # Main log (INFO+)
    ‚îî‚îÄ‚îÄ scraper_errors.log       # Error log only
```

### CSV Schema

| Column | Type | Description |
|--------|------|-------------|
| `Title` | str | Property title |
| `URL` | str | Listing URL (unique) |
| `City` | str | NCR city |
| `Location` | str | Locality/society name |
| `Price_Raw` | str | Original price text |
| `Price` | float | Numeric price (‚Çπ) |
| `Area` | str | Property area |
| `Prop_Type` | str | Apartment/Villa/Plot/Builder Floor |
| `Bedrooms` | int | Number of bedrooms |
| `Bathrooms` | int | Number of bathrooms |
| `Balcony` | int | Number of balconies |
| `Floor` | int | Floor number |
| `Facing` | str | Direction (North/South/East/West) |
| `Furnished` | str | Fully/Semi/Unfurnished |
| `Pooja_Room` | int | 1 if present, 0 otherwise |
| `Servant_Room` | int | 1 if present, 0 otherwise |
| `Store_Room` | int | 1 if present, 0 otherwise |
| `Pool` | int | 1 if present, 0 otherwise |
| `Gym` | int | 1 if present, 0 otherwise |
| `Lift` | int | 1 if present, 0 otherwise |
| `Parking` | int | 1 if present, 0 otherwise |
| `Vastu_Compliant` | int | 1 if mentioned, 0 otherwise |

## Architecture

### Class Structure

```
PropertyScraper
‚îú‚îÄ‚îÄ CheckpointManager    # Progress tracking
‚îú‚îÄ‚îÄ DataBuffer           # Batched I/O
‚îî‚îÄ‚îÄ Methods
    ‚îú‚îÄ‚îÄ _setup_driver()           # Simple uc.Chrome init
    ‚îú‚îÄ‚îÄ _load_cookies()           # Session management
    ‚îú‚îÄ‚îÄ _simulate_human_behavior() # Anti-bot delays
    ‚îú‚îÄ‚îÄ _handle_soft_ban()        # Ban recovery
    ‚îú‚îÄ‚îÄ _extract_data()           # Feature extraction
    ‚îî‚îÄ‚îÄ _scrape_city()            # City scraping loop
```

### Design Principles

1. **Simplicity over abstraction**: No over-engineering that breaks `undetected_chromedriver`
2. **Survival over speed**: Random delays preserved for anti-bot evasion
3. **Batched I/O**: Reduces disk writes while ensuring data safety
4. **Graceful degradation**: Handles errors without crashing

## Troubleshooting

### "Access Denied" / Soft Ban

**Symptom**: Page shows "Access Denied"

**Solution**: The scraper automatically waits 2 minutes and retries. If persistent:
- Ensure cookies are loaded
- Reduce `coffee_break_interval` (take breaks more often)
- Increase random delay ranges in `_simulate_human_behavior()`

### Driver Crashes

**Symptom**: Chrome crashes or becomes unresponsive

**Solution**: The scraper auto-restarts the driver. If frequent:
- Update `undetected-chromedriver`: `pip install -U undetected-chromedriver`
- Update Chrome browser to latest version
- Check system resources (RAM/CPU)

### No Data Extracted

**Symptom**: Pages load but no listings saved

**Solution**:
- Check if 99acres changed their HTML structure
- Enable DEBUG logging: Set `logging.level: "DEBUG"` in config
- Inspect `logs/scraper.log` for parsing errors

### Internet Connection Lost

**Symptom**: "Internet Lost! Pausing script..."

**Solution**: The scraper auto-pauses and resumes when connection returns. No action needed.

## Best Practices

### üöÄ For Maximum Data Collection

1. **Use cookies**: Export from logged-in session
2. **Run overnight**: Scraping 56K listings takes ~8-12 hours
3. **Monitor logs**: Check `logs/scraper.log` periodically
4. **Don't modify delays**: Random delays are calibrated for survival

### ‚ö†Ô∏è What NOT to Do

1. ‚ùå **Don't reduce random delays** - Instant ban risk
2. ‚ùå **Don't use headless mode** - 99acres detects it
3. ‚ùå **Don't run multiple instances** - IP flagging risk
4. ‚ùå **Don't modify driver setup** - Breaks `undetected_chromedriver`

## Performance

### Expected Metrics

- **Speed**: ~50-100 listings/minute (varies by page load)
- **Success Rate**: ~95% (with proper cookie setup)
- **Total Time**: 8-12 hours for 56K listings (7 cities √ó 8K each)
- **Data Quality**: ~98% valid prices (rest filtered out)

### Resource Usage

- **RAM**: ~500MB (Chrome) + ~100MB (Python)
- **Disk**: ~50MB final CSV (56K listings)
- **Network**: ~2-5 GB total (HTML + images)

## Code Quality

### Type Hints

All functions have type annotations:
```python
def clean_price(price_text: Optional[str]) -> Optional[float]:
    ...
```

### Documentation

Every method has comprehensive docstrings explaining:
- Purpose
- Arguments
- Return values
- **WHY delays exist** (critical for maintainers)

### PEP 8 Compliance

Code follows PEP 8 except for intentional "messy" delays:
```python
time.sleep(random.uniform(1.5, 3))  # KEEP RANDOM - anti-bot
```

## License

MIT License - See LICENSE file for details.

## Contributing

When modifying this scraper, remember:

1. **Preserve all random delays** - They're not bugs, they're features
2. **Test with cookies** - Anonymous scraping has higher ban risk
3. **Monitor logs** - Check for new anti-bot measures
4. **Document WHY** - Explain rationale for "ugly" code

---

**Happy Scraping! üè†üìä**
